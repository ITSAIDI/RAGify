{"cells":[{"cell_type":"markdown","source":["# Introduction"],"metadata":{"id":"NXP8VGf25cw3"}},{"cell_type":"markdown","metadata":{"id":"X_IYiQ2v5Pkq"},"source":["- In this Notebook we will build and test a simple RAG System using LangChain and Ollama.\n","- We will use a small Corpus as the New Knowledge we RAG on it."]},{"cell_type":"markdown","source":["# Dependencies :"],"metadata":{"id":"NO40zoun5l2P"}},{"cell_type":"code","source":["!pip install -qU langchain langchain_community\n","!pip install -qU langchain_chroma\n","!pip install -qU langchain_ollama"],"metadata":{"id":"wgE-AHdr5rZG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- To work with Ollama on Colab, we have an additional library, so we can launch a terminal just like in our local machine !"],"metadata":{"id":"ALbHWl5M6Xho"}},{"cell_type":"markdown","source":["1. `pciutils` is required by Ollama to detect the GPU type.\n","2. Installation of Ollama in the runtime instance will be taken care by `curl -fsSL https://ollama.com/install.sh | sh`\n","\n","\n"],"metadata":{"id":"B1S1YL6EnYBB"}},{"cell_type":"code","source":["!sudo apt update\n","!sudo apt install -y pciutils\n","!curl -fsSL https://ollama.com/install.sh | sh"],"metadata":{"id":"YlVK9iG4AD5L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Implimentation :"],"metadata":{"id":"z5DkxD9K9XOd"}},{"cell_type":"markdown","source":["## Running Ollama\n","---\n","\n","In order to use Ollama it needs to run as a service in background parallel to your scripts. Becasue Jupyter Notebooks is built to run code blocks in sequence this make it difficult to run two blocks at the same time. As a workaround we will create a service using subprocess in Python so it doesn't block any cell from running.\n","\n","Service can be started by command `ollama serve`.\n","\n","`time.sleep(5)` adds some delay to get the Ollama service up before downloading the model."],"metadata":{"id":"fGEJwjTPoKWH"}},{"cell_type":"code","source":["import threading\n","import subprocess\n","import time\n","\n","def run_ollama_serve():\n","  subprocess.Popen([\"ollama\", \"serve\"])\n","\n","thread = threading.Thread(target=run_ollama_serve)\n","thread.start()\n","time.sleep(5)"],"metadata":{"id":"Jh5CBAFxBYAC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Pulling Model\n","---\n","\n","Download the LLM model using `ollama pull llama3.2`.\n","\n","For other models check https://ollama.com/library"],"metadata":{"id":"WcBLqZfyoHg4"}},{"cell_type":"code","source":["!ollama pull llama3.1:8b"],"metadata":{"id":"o2ghppmRDFny"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ollama list"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fG3DclthccOJ","executionInfo":{"status":"ok","timestamp":1732644179129,"user_tz":-60,"elapsed":242,"user":{"displayName":"Mono job","userId":"13971337091366788927"}},"outputId":"bc9f148b-fb36-4817-d722-63b2afe61259"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["NAME           ID              SIZE      MODIFIED      \n","llama3.1:8b    42182419e950    4.7 GB    8 seconds ago    \n"]}]},{"cell_type":"markdown","source":["## Build"],"metadata":{"id":"h613FcaMoaf2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"HnkegBbu5Pk2"},"outputs":[],"source":["from langchain_community.llms import Ollama\n","from langchain_community.embeddings import OllamaEmbeddings\n","from langchain.document_loaders import TextLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.vectorstores import Chroma\n","from langchain.chains import create_retrieval_chain\n","from langchain.chains.combine_documents import create_stuff_documents_chain\n","from langchain import hub\n"]},{"cell_type":"code","source":["model_name = \"llama3.1:8b\""],"metadata":{"id":"WrGiIN7X9tSs"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K6bzAt4c5Pk6"},"outputs":[],"source":["llm = Ollama(model=model_name,base_url = \"http://localhost:11434\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LtFkY-OB5Pk8"},"outputs":[],"source":["embed_model = OllamaEmbeddings(\n","    model=model_name,\n","    base_url=\"http://localhost:11434\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bx7adXVM5Pk9"},"outputs":[],"source":["text=\"\"\"\n","### The Mysterious Visitor of Chefchaouen\n","\n","In the small village of Chefchaouen, nestled in the heart of the Rif Mountains, lived a young girl named **Lamia**, known for her kindness and radiant smile. She spent her days helping her mother sell traditional rugs at the town’s bustling souk. One day, as she managed the stall alone, a stranger in a white djellaba, named **Omar**, appeared. His accent revealed he was from a distant region, perhaps Marrakech.\n","\n","Omar admired a rug woven with intricate red and blue patterns. “This design tells an ancient story, doesn’t it?” he asked. Lamia, curious, replied, “Yes, every pattern holds a secret of our culture. But how do you know this?” The man smiled mysteriously and said, “Because I’ve been searching for one like this for a long time.”\n","\n","Intrigued, Lamia offered him a cup of mint tea. As they talked, he explained that he was looking for a specific rug with a key-shaped motif. According to legend, this rug would unlock the entrance to a hidden cave containing a forgotten treasure. Lamia then remembered an old rug her grandmother had woven years ago, kept safely in their home.\n","\n","That evening, she showed the rug to Omar. To her astonishment, he exclaimed, “This is the one!” The next day, they set out together into the mountains. After hours of trekking, they arrived at a cave whose entrance was concealed by thorny bushes. Omar unrolled the rug in front of the entrance, and a soft light illuminated the interior. Inside, they discovered chests brimming with ancient jewelry and gold coins.\n","\n","Omar revealed he wasn’t a treasure hunter but a historian who wanted to preserve this heritage for future generations. Touched by his sincerity, Lamia agreed to help him. Together, they brought some of the treasures back to the village, enriching the community and safeguarding history.\n","\n","From that day on, Omar and Lamia became inseparable, united by their love for the traditions and mysteries of Morocco.\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fk4EqT9s5PlA"},"outputs":[],"source":["text_splitter = RecursiveCharacterTextSplitter(chunk_size = 512, chunk_overlap = 128)\n","chunks = text_splitter.split_text(text)\n","vector_store = Chroma.from_texts(chunks, embed_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"buRlPPcg5PlD"},"outputs":[],"source":["retriever = vector_store.as_retriever()\n","chain = create_retrieval_chain(combine_docs_chain = llm , retriever = retriever)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mLp4QMWU5PlE","executionInfo":{"status":"ok","timestamp":1732644276549,"user_tz":-60,"elapsed":244,"user":{"displayName":"Mono job","userId":"13971337091366788927"}},"outputId":"17bd737f-d0f4-4d6c-81a4-4b861c457312"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n","  warnings.warn(\n"]}],"source":["retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I-DDuqIQ5PlH"},"outputs":[],"source":["combine_docs_chain = create_stuff_documents_chain( llm, retrieval_qa_chat_prompt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HEVkKTor5PlI"},"outputs":[],"source":["retrival_chain = create_retrieval_chain(retriever,combine_docs_chain)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4wP9nlmN5PlI","executionInfo":{"status":"ok","timestamp":1732644284677,"user_tz":-60,"elapsed":2038,"user":{"displayName":"Mono job","userId":"13971337091366788927"}},"outputId":"1e89a8cd-8dcf-4cff-fcad-6701c073a53c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Lamia is the name of the girl in the story.\n"]}],"source":["response = retrival_chain.invoke({\"input\":\"What is the name of the girl in the Story ?\"})\n","print(response[\"answer\"])"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.15"},"colab":{"provenance":[],"collapsed_sections":["NXP8VGf25cw3","NO40zoun5l2P","z5DkxD9K9XOd","fGEJwjTPoKWH","WcBLqZfyoHg4","h613FcaMoaf2"],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}